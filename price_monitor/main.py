import argparse
from pathlib import Path
import pandas as pd
import yaml
import os

from price_monitor.scrapers.bs4_scraper import scrape_bs4
from price_monitor.scrapers.selenium_scraper import scrape_selenium
from price_monitor.scrapers.scrapy_runner import scrape_with_scrapy
from price_monitor.matching import match_competitors_to_catalog
from price_monitor.compare import build_price_comparison
from price_monitor.recommend import build_recommendations

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞
ROOT = Path(__file__).resolve().parents[1]
OUT = ROOT / "out"
CFG = ROOT / "config"
DATA = ROOT / "data"

def load_yaml(path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ YAML-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def ensure_dirs():
    """–°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
    OUT.mkdir(exist_ok=True, parents=True)

def cmd_scrape(args):
    """–ö–æ–º–∞–Ω–¥–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–∞–π—Ç–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤"""
    print("="*50)
    print("–ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ —Ü–µ–Ω –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤...")
    print("="*50)
    
    ensure_dirs()
    cfg = load_yaml(CFG / "sites.yaml")
    all_rows = []
    
    for site in cfg["sites"]:
        t = site["type"].lower()
        print(f"\n[–ü–ê–†–°–ò–ù–ì] {site['name']} ({t})")
        try:
            if t == "bs4":
                rows = scrape_bs4(site)
            elif t == "selenium":
                rows = scrape_selenium(site)
            elif t == "scrapy":
                rows = scrape_with_scrapy(site)
            else:
                print(f"  ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø–∞—Ä—Å–µ—Ä–∞: {t}")
                rows = []
            all_rows.extend(rows)
            print(f"  ‚úÖ –ù–∞–π–¥–µ–Ω–æ –ø–æ–∑–∏—Ü–∏–π: {len(rows)}")
        except Exception as e:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site['name']}: {str(e)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    df = pd.DataFrame(all_rows)
    if not df.empty:
        df["price"] = pd.to_numeric(df["price"], errors="coerce")
        df = df.dropna(subset=["price"])
        output_path = OUT / "scraped_prices.csv"
        df.to_csv(output_path, index=False, encoding="utf-8")
        print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –≤ {output_path}")
    else:
        print("\n‚ö†Ô∏è –ù–µ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ü–µ–Ω—ã!")
    return df

def cmd_analyze(args):
    """–ö–æ–º–∞–Ω–¥–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω"""
    print("\n" + "="*50)
    print("–ê–Ω–∞–ª–∏–∑ –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω...")
    print("="*50)
    
    ensure_dirs()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    catalog_path = DATA / "internal_catalog.csv"
    scraped_path = OUT / "scraped_prices.csv"
    
    if not scraped_path.exists():
        print("‚ùå –§–∞–π–ª —Å —Ü–µ–Ω–∞–º–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥.")
        return
        
    catalog = pd.read_csv(catalog_path)
    scraped = pd.read_csv(scraped_path)
    pricing_cfg = load_yaml(CFG / "pricing.yaml")
    
    # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    matched = match_competitors_to_catalog(scraped, catalog, pricing_cfg)
    if not matched.empty:
        matched_path = OUT / "matched.csv"
        matched.to_csv(matched_path, index=False, encoding="utf-8")
        print(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ {len(matched)} –ø–æ–∑–∏—Ü–∏–π. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {matched_path}")
    else:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏")
        return
        
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω
    comp = build_price_comparison(matched, catalog)
    if not comp.empty:
        comp_path = OUT / "comparison.csv"
        comp.to_csv(comp_path, index=False, encoding="utf-8")
        print(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {comp_path}")
        
        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
        print("\n–°–≤–æ–¥–∫–∞ –ø–æ –ø–æ–∑–∏—Ü–∏—è–º:")
        for _, row in comp.iterrows():
            status = "‚úÖ –í—ã–≥–æ–¥–Ω–æ" if row["position"] == "cheapest" else "‚ö†Ô∏è –í—ã—à–µ —Ä—ã–Ω–∫–∞"
            print(f"{row['name']} (SKU: {row['sku']}):")
            print(f"  –ù–∞—à–∞ —Ü–µ–Ω–∞: {row['current_price']}‚ÇΩ | –ú–∏–Ω. –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç: {row['min_comp_price']}‚ÇΩ")
            print(f"  –ü–æ–∑–∏—Ü–∏—è: {status} | –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤: {row['competitors']}")
    else:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å —Ü–µ–Ω—ã")

def cmd_recommend(args):
    """–ö–æ–º–∞–Ω–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ü–µ–Ω–∞–º"""
    print("\n" + "="*50)
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ü–µ–Ω–∞–º...")
    print("="*50)
    
    ensure_dirs()
    comp_path = OUT / "comparison.csv"
    
    if not comp_path.exists():
        print("‚ùå –§–∞–π–ª —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑.")
        return
        
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    catalog = pd.read_csv(DATA / "internal_catalog.csv")
    comparison = pd.read_csv(comp_path)
    pricing_cfg = load_yaml(CFG / "pricing.yaml")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    recs = build_recommendations(comparison, catalog, pricing_cfg)
    if not recs.empty:
        recs_path = OUT / "recommendations.csv"
        recs.to_csv(recs_path, index=False, encoding="utf-8")
        
        # –í—ã–≤–æ–¥ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ü–µ–Ω–∞–º:")
        for _, row in recs.iterrows():
            action_icon = "‚¨áÔ∏è –°–Ω–∏–∑–∏—Ç—å" if row["action"] == "decrease" else "‚¨ÜÔ∏è –ü–æ–≤—ã—Å–∏—Ç—å" if row["action"] == "increase" else "üîÑ –û—Å—Ç–∞–≤–∏—Ç—å"
            print(f"{row['sku']}: {action_icon} —Å {row['current_price']}‚ÇΩ –¥–æ {row['recommended_price']}‚ÇΩ")
            print(f"  –ü—Ä–∏—á–∏–Ω–∞: {row['reason']}")
        
        print(f"\n–ü–æ–ª–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {recs_path}")
    else:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

def cmd_run_all(args):
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ —ç—Ç–∞–ø—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ"""
    cmd_scrape(args)
    cmd_analyze(args)
    cmd_recommend(args)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥"""
    parser = argparse.ArgumentParser(
        description="–°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ü–µ–Ω –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    subparsers = parser.add_subparsers(title="–ö–æ–º–∞–Ω–¥—ã", dest="command", required=True)

    # –ü–∞—Ä—Å–∏–Ω–≥
    scrape_parser = subparsers.add_parser("scrape", help="–°–æ–±—Ä–∞—Ç—å —Ü–µ–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
    scrape_parser.set_defaults(func=cmd_scrape)

    # –ê–Ω–∞–ª–∏–∑
    analyze_parser = subparsers.add_parser("analyze", help="–°–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ü–µ–Ω—ã")
    analyze_parser.set_defaults(func=cmd_analyze)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    recommend_parser = subparsers.add_parser("recommend", help="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ü–µ–Ω–∞–º")
    recommend_parser.set_defaults(func=cmd_recommend)

    # –í—Å–µ —ç—Ç–∞–ø—ã
    all_parser = subparsers.add_parser("run-all", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –≤—Å–µ —ç—Ç–∞–ø—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ")
    all_parser.set_defaults(func=cmd_run_all)

    args = parser.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()